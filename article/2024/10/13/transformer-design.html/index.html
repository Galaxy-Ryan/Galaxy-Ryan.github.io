<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="Galaxy Ryan"><meta name="copyright" content="Galaxy Ryan"><meta name="generator" content="Hexo 7.3.0"><meta name="theme" content="hexo-theme-yun"><title>Transformer的网络架构实现 | 银河的白星</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/star-markdown-css@0.4.1/dist/yun/yun-markdown.min.css"><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/prism-theme-vars/base.css"><script src="https://fastly.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".markdown-body img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="icon" type="image/png" href="/favicon.ico"><link rel="mask-icon" href="/favicon.ico" color="#0078E7"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="preconnect" href="https://fastly.jsdelivr.net/npm/" crossorigin><script id="yun-config">
    window.Yun = {}
    window.CONFIG = {"hostname":"galaxy-ryan.github.io","root":"/","title":"银河的白星","version":"1.10.11","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"搜索...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）"},"anonymous_image":"https://cdn.yunyoujun.cn/img/avatar/none.jpg","say":{"api":"https://el-bot-api.vercel.app/api/words/young"},"fireworks":{"colors":null},"vendors":{"host":"https://fastly.jsdelivr.net/npm/","darken":"https://fastly.jsdelivr.net/npm/darken@1.5.0"}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/hexo-theme-yun.js" type="module"></script><link rel="alternate" href="/atom.xml" title="银河的白星" type="application/atom+xml"><meta name="description" content="在这里，我们通过Google论文《Attention is all you need》中给出的方法，利用Pytorch框架给出的轮子来实现Transformer的整体网络架构。">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer的网络架构实现">
<meta property="og:url" content="https://galaxy-ryan.github.io/article/2024/10/13/transformer-design.html/index.html">
<meta property="og:site_name" content="银河的白星">
<meta property="og:description" content="在这里，我们通过Google论文《Attention is all you need》中给出的方法，利用Pytorch框架给出的轮子来实现Transformer的整体网络架构。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://galaxy-ryan.github.io/article/2024/10/13/transformer-design.html/1.png">
<meta property="og:image" content="https://galaxy-ryan.github.io/article/2024/10/13/transformer-design.html/2.png">
<meta property="og:image" content="https://galaxy-ryan.github.io/article/2024/10/13/transformer-design.html/3.png">
<meta property="og:image" content="https://galaxy-ryan.github.io/article/2024/10/13/transformer-design.html/5.png">
<meta property="og:image" content="https://galaxy-ryan.github.io/article/2024/10/13/transformer-design.html/4.png">
<meta property="og:image" content="https://galaxy-ryan.github.io/article/2024/10/13/transformer-design.html/6.png">
<meta property="og:image" content="https://galaxy-ryan.github.io/article/2024/10/13/transformer-design.html/7.png">
<meta property="og:image" content="https://galaxy-ryan.github.io/article/2024/10/13/transformer-design.html/8.png">
<meta property="og:image" content="https://galaxy-ryan.github.io/article/2024/10/13/transformer-design.html/9.png">
<meta property="article:published_time" content="2024-10-13T04:50:42.000Z">
<meta property="article:modified_time" content="2024-10-13T04:50:42.000Z">
<meta property="article:author" content="Galaxy Ryan">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://galaxy-ryan.github.io/article/2024/10/13/transformer-design.html/1.png"><script>(function() {
  if (CONFIG.mode !== 'auto') return
  const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches
  const setting = localStorage.getItem('darken-mode') || 'auto'
  if (setting === 'dark' || (prefersDark && setting !== 'light'))
    document.documentElement.classList.toggle('dark', true)
})()</script></head><body><script src="https://code.iconify.design/2/2.1.1/iconify.min.js"></script><script>// Define global variable
IconifyProviders = {
  // Empty prefix: overwrite default API provider configuration
  '': {
    // Use custom API first, use Iconify public API as backup
    resources: [
        'https://api.iconify.design',
    ],
    // Wait for 1 second before switching API hosts
    rotate: 1000,
  },
};</script><script defer src="https://fastly.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js" type="module"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js" type="module"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><span class="icon iconify" data-icon="ri:list-ordered"></span></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><span class="icon iconify" data-icon="ri:passport-line"></span></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="Galaxy Ryan"><img width="96" loading="lazy" src="/images/avatar.jpg" alt="Galaxy Ryan"><span class="site-author-status" title="烟火璀璨">☺️</span></a><div class="site-author-name"><a href="/about/">Galaxy Ryan</a></div><span class="site-name">银河的白星</span><sub class="site-subtitle">一个不知名的个人博客</sub><div class="site-description">云散月明谁点缀，天容海色本澄清</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:home-4-line"></span></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:archive-line"></span></span><span class="site-state-item-count">15</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:folder-2-line"></span></span><span class="site-state-item-count">3</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="site-state-item-count">11</span></a></div><a class="site-state-item hty-icon-button" target="_blank" rel="noopener" href="https://yun.yunyoujun.cn" title="文档"><span class="site-state-item-icon"><span class="icon iconify" data-icon="ri:settings-line"></span></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><span class="icon iconify" data-icon="ri:rss-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/Galaxy-Ryan" title="GitHub" target="_blank" style="color:#181717"><span class="icon iconify" data-icon="ri:github-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:ryan_williams@163.com" title="E-Mail" target="_blank" style="color:#8E71C1"><span class="icon iconify" data-icon="ri:mail-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://music.163.com" title="网易云音乐" target="_blank" style="color:#C10D0C"><span class="icon iconify" data-icon="ri:netease-cloud-music-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://www.zhihu.com" title="知乎" target="_blank" style="color:#0084FF"><span class="icon iconify" data-icon="ri:zhihu-line"></span></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><span class="icon iconify" data-icon="ri:bilibili-line"></span></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="友情链接" style="color:dodgerblue"><span class="icon iconify" data-icon="ri:genderless-line"></span></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><span class="icon iconify" data-icon="ri:contrast-2-line"></span></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer%E7%9A%84%E6%95%B4%E4%BD%93%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc-number">1.</span> <span class="toc-text">Transformer的整体网络架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Embedding-%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.</span> <span class="toc-text">Embedding 实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text">多头注意力机制的实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer%E5%AE%9E%E7%8E%B0"><span class="toc-number">4.</span> <span class="toc-text">Transformer实现</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article" style="--smc-primary:#0078E7;"><link itemprop="mainEntityOfPage" href="https://Galaxy-Ryan.github.io/article/2024/10/13/transformer-design.html/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="Galaxy Ryan"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="银河的白星"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Transformer的网络架构实现</h1><div class="post-meta"><div class="post-time"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:calendar-line"></span></span> <time title="创建时间：2024-10-13 12:50:42" itemprop="dateCreated datePublished" datetime="2024-10-13T12:50:42+08:00">2024-10-13</time></div><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><span class="icon iconify" data-icon="ri:folder-line"></span></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/%E6%8A%80%E6%9C%AF/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">技术</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/AI/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><span class="icon iconify" data-icon="ri:price-tag-3-line"></span></span><span class="tag-name">AI</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body"><p>在这里，我们通过Google论文《Attention is all you need》中给出的方法，利用Pytorch框架给出的轮子来实现Transformer的整体网络架构。</p>
<span id="more"></span>

<h2 id="Transformer的整体网络架构"><a href="#Transformer的整体网络架构" class="headerlink" title="Transformer的整体网络架构"></a>Transformer的整体网络架构</h2><p>让我们来看看Transformer的整体网络架构：整个Transformer网络包含左右两个部分，即Encoder和Decoder。输入数据通过token Embedding和位置编码后进入编码器和解码器网络结构，解码器最后通过线性层和Softmax操作完成一次输出。而解码器之前时刻的输出就又可以作为下一时候的输入（Shifted Right）。</p>
<p><img src="1.png" alt="1" loading="lazy"></p>
<p>对于Encoder部分来说其内部主要以6个相同的模块堆叠而成，每一个模块由两部分网络所构成：多头注意力机制和两层前馈神经网络。同时，对于这两部分网络来说，都加入了残差连接，并且在残差连接后还进行了层归一化操作。同样，Decoder层也采用了6个完全相同的网络层堆叠而成，其整体上与Encoder类似，只是多了一个用于与Encoder输出进行交互的多头注意力机制。</p>
<h2 id="Embedding-实现"><a href="#Embedding-实现" class="headerlink" title="Embedding 实现"></a>Embedding 实现</h2><ul>
<li><p><strong>Token Embedding</strong>：这里是实现字符转向量的一种常用做法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TokenEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, emb_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(TokenEmbedding, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, emb_size)</span><br><span class="line">        <span class="variable language_">self</span>.emb_size = emb_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.embedding(tokens.long()) * math.sqrt(<span class="variable language_">self</span>.emb_size)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Positional Embedding</strong>：位置编码，按照论文中给出的以下公式，并将含有指数部分的参数进行对数变换。</p>
<p><img src="2.png" alt="2" loading="lazy"></p>
<p>变换后的结果如下：</p>
<p><img src="3.png" alt="3" loading="lazy"></p>
<p>下面我们进行位置编码的实现，首先初始化一个全0的位置矩阵，并指定了一个序列的最大长度，然后计算每个维度（每一列）的相关位置信息，并在位置矩阵中取与输入序列长度相等的前<code>x_len</code>行，再加上Token Embedding的结果，最后返回矩阵进行Dropout操作后得到的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        pe = torch.zeros(max_len, d_model) <span class="comment"># [max_len, d_model]</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>) <span class="comment"># [max_len, 1]</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        <span class="comment"># [d_model/2]</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term) <span class="comment"># [max_len, d_model/2]</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># [max_len, 1, d_model]</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + <span class="variable language_">self</span>.pe[:x.size(<span class="number">0</span>), :]  <span class="comment"># [x_len, batch_size, d_model]</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.dropout(x)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="多头注意力机制的实现"><a href="#多头注意力机制的实现" class="headerlink" title="多头注意力机制的实现"></a>多头注意力机制的实现</h2><p><img src="5.png" alt="5" loading="lazy"></p>
<p>如上图所示，多头注意力机制其实就是将原始的输入序列进行多组的自注意力处理过程；然后再将每一组自注意力的结果拼接起来进行一次线性变换得到最终的输出结果。具体的，其计算公式为：</p>
<p><img src="4.png" alt="4" loading="lazy"></p>
<p>其中，</p>
<p><img src="6.png" alt="6" loading="lazy"></p>
<p>并且，</p>
<p><img src="7.png" alt="7" loading="lazy"></p>
<p>多头注意力机制中最为重要的就是自注意力机制，也就是需要前计算得到Q、K和V，然后再根据Q、K、V来计算得到最终的注意力编码。所谓的自注意力机制其实就是论文中所指代的“Scaled Dot-Product Attention“。</p>
<p><img src="8.png" alt="8" loading="lazy"></p>
<p>在论文中作者说道，注意力机制可以描述为将query和一系列的key-value对映射到某个输出的过程，而这个输出的向量就是根据query和key计算得到的权重作用于value上的权重和。具体的，对于输入Q、K和V来说，其输出向量的计算公式为：</p>
<p><img src="9.png" alt="9" loading="lazy"></p>
<p>下面我们进行多头注意力的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_dim, num_heads, dropout=<span class="number">0.</span>, bias=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.embed_dim = embed_dim  <span class="comment"># 前面的d_model参数</span></span><br><span class="line">        <span class="variable language_">self</span>.head_dim = embed_dim // num_heads  <span class="comment"># head_dim 指的就是d_k,d_v</span></span><br><span class="line">        <span class="variable language_">self</span>.kdim = <span class="variable language_">self</span>.head_dim</span><br><span class="line">        <span class="variable language_">self</span>.vdim = <span class="variable language_">self</span>.head_dim</span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads  <span class="comment"># 多头个数</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = dropout</span><br><span class="line">        <span class="keyword">assert</span> <span class="variable language_">self</span>.head_dim * num_heads == <span class="variable language_">self</span>.embed_dim</span><br><span class="line">        <span class="comment"># 上面的限制条件就是论文中的  d_k = d_v = d_model/n_head 条件</span></span><br><span class="line">        <span class="variable language_">self</span>.q_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))  </span><br><span class="line">        <span class="comment"># embed_dim = kdim * num_heads</span></span><br><span class="line">        <span class="comment"># 这里第二个维度之所以是embed_dim，实际上这里是同时初始化了num_heads个W_q堆叠起来的, 也就是num_heads个头</span></span><br><span class="line">        <span class="variable language_">self</span>.k_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))  </span><br><span class="line">        <span class="comment"># W_k,  embed_dim = kdim * num_heads</span></span><br><span class="line">        <span class="variable language_">self</span>.v_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))  </span><br><span class="line">        <span class="comment"># W_v,  embed_dim = vdim * num_heads</span></span><br><span class="line">        <span class="variable language_">self</span>.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)</span><br><span class="line">        <span class="comment"># 最后将所有的Z组合起来的时候，也是一次性完成， embed_dim = vdim * num_heads</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, attn_mask=<span class="literal">None</span>, key_padding_mask=<span class="literal">None</span></span>):</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>

<p>接下来，我们分五个阶段定义多头注意力机制前向传播的实现：</p>
<ul>
<li><p><strong>第一阶段： 计算得到Q、K、V</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一阶段： 计算得到Q、K、V</span></span><br><span class="line">	q = F.linear(query, <span class="variable language_">self</span>.q_proj_weight)</span><br><span class="line">	k = F.linear(key, <span class="variable language_">self</span>.k_proj_weight)</span><br><span class="line">	v = F.linear(value, <span class="variable language_">self</span>.v_proj_weight)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>第二阶段： 缩放，以及attn_mask维度判断</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二阶段： 缩放，以及attn_mask维度判断</span></span><br><span class="line">    tgt_len, bsz, embed_dim = query.size()</span><br><span class="line">    src_len = key.size(<span class="number">0</span>)</span><br><span class="line">    head_dim = embed_dim // <span class="variable language_">self</span>.num_heads</span><br><span class="line">    scaling = <span class="built_in">float</span>(head_dim) ** -<span class="number">0.5</span></span><br><span class="line">    q = q * scaling</span><br><span class="line">    <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:  </span><br><span class="line">        <span class="keyword">if</span> attn_mask.dim() == <span class="number">2</span>:</span><br><span class="line">            attn_mask = attn_mask.unsqueeze(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">list</span>(attn_mask.size()) != [<span class="number">1</span>, query.size(<span class="number">0</span>), key.size(<span class="number">0</span>)]:</span><br><span class="line">                <span class="keyword">raise</span> RuntimeError(<span class="string">&#x27;The size of the 2D attn_mask is not correct.&#x27;</span>)</span><br><span class="line">        <span class="keyword">elif</span> attn_mask.dim() == <span class="number">3</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">list</span>(attn_mask.size()) != [bsz * <span class="variable language_">self</span>.num_heads, query.size(<span class="number">0</span>), key.size(<span class="number">0</span>)]:</span><br><span class="line">                <span class="keyword">raise</span> RuntimeError(<span class="string">&#x27;The size of the 3D attn_mask is not correct.&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>第三阶段： 计算得到注意力权重矩阵</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第三阶段： 计算得到注意力权重矩阵</span></span><br><span class="line">    q = q.contiguous().view(tgt_len, bsz * <span class="variable language_">self</span>.num_heads, head_dim).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># [batch_size * num_heads,tgt_len,kdim]</span></span><br><span class="line">    <span class="comment"># 因为前面是num_heads个头一起参与的计算，所以这里要进行一下变形，以便于后面计算。 且同时交换了0，1两个维度</span></span><br><span class="line">    k = k.contiguous().view(-<span class="number">1</span>, bsz*<span class="variable language_">self</span>.num_heads, head_dim).transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#[batch_size * num_heads,src_len,kdim]</span></span><br><span class="line">    v = v.contiguous().view(-<span class="number">1</span>, bsz*<span class="variable language_">self</span>.num_heads, head_dim).transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#[batch_size * num_heads,src_len,vdim]</span></span><br><span class="line">    attn_output_weights = torch.bmm(q, k.transpose(<span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>第四阶段： 进行相关掩码操作</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第四阶段： 进行相关掩码操作</span></span><br><span class="line">    <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        attn_output_weights += attn_mask</span><br><span class="line">    <span class="keyword">if</span> key_padding_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        attn_output_weights = attn_output_weights.view(bsz, <span class="variable language_">self</span>.num_heads, tgt_len, src_len)</span><br><span class="line">        attn_output_weights = attn_output_weights.masked_fill(</span><br><span class="line">            key_padding_mask.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>), <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)) </span><br><span class="line">        attn_output_weights = attn_output_weights.view(bsz * <span class="variable language_">self</span>.num_heads, tgt_len,src_len)  </span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>最后，对权重矩阵进行归一化操作，并计算得到多头注意力机制的输出</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">attn_output_weights = F.softmax(attn_output_weights, dim=-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># [batch_size * num_heads, tgt_len, src_len]</span></span><br><span class="line">attn_output_weights = F.dropout(attn_output_weights, p=<span class="variable language_">self</span>.dropout, training=<span class="variable language_">self</span>.training)</span><br><span class="line">attn_output = torch.bmm(attn_output_weights, v)</span><br><span class="line"><span class="comment"># 这就是num_heads个Attention(Q,K,V)结果</span></span><br><span class="line">  </span><br><span class="line">attn_output = attn_output.transpose(<span class="number">0</span>, <span class="number">1</span>).contiguous().view(tgt_len, bsz, embed_dim)</span><br><span class="line">attn_output_weights = attn_output_weights.view(bsz, <span class="variable language_">self</span>.num_heads, tgt_len, src_len)</span><br><span class="line">  </span><br><span class="line">Z = F.linear(attn_output, <span class="variable language_">self</span>.out_proj.weight, <span class="variable language_">self</span>.out_proj.bias)</span><br><span class="line"><span class="comment"># 这里就是多个z  线性组合成Z  [tgt_len,batch_size,embed_dim]</span></span><br><span class="line"><span class="keyword">return</span> Z, attn_output_weights.<span class="built_in">sum</span>(dim=<span class="number">1</span>) / <span class="variable language_">self</span>.num_heads  <span class="comment"># 将num_heads个注意力权重矩阵按对应维度取平均</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Transformer实现"><a href="#Transformer实现" class="headerlink" title="Transformer实现"></a>Transformer实现</h2><p>在完成<strong>Embedding</strong>和<strong>MultiHeadedAttention</strong>部分的设计后，我们就可以正式搭建Transformer的网络结构。对于Transformer网络的实现一共会包含4个部分：<code>TransformerEncoderLayer</code>、<code>TransformerEncoder</code>、<code>TransformerDecoderLayer</code>和<code>TransformerDecoder</code>，其分别表示定义一个单独编码层、构造由多个编码层组合得到的编码器、定义一个单独的解码层以及构造由多个解码层得到的解码器。</p>
<ul>
<li><p><strong>TransformerEncoderLayer层实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, nhead, dim_feedforward=<span class="number">2048</span>, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.self_attn = MultiHeadedAttention(d_model, nhead, dropout=dropout)</span><br><span class="line">        <span class="variable language_">self</span>.dropout1 = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.norm1 = nn.LayerNorm(d_model)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.linear1 = nn.Linear(d_model, dim_feedforward)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.linear2 = nn.Linear(dim_feedforward, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.activation = F.relu</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.dropout2 = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.norm2 = nn.LayerNorm(d_model)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, src_mask=<span class="literal">None</span>, src_key_padding_mask=<span class="literal">None</span></span>):</span><br><span class="line">        src2 = <span class="variable language_">self</span>.self_attn(src, src, src, attn_mask=src_mask,</span><br><span class="line">                              key_padding_mask=src_key_padding_mask)[<span class="number">0</span>]  <span class="comment"># 计算多头注意力</span></span><br><span class="line">        src = src + <span class="variable language_">self</span>.dropout1(src2)  <span class="comment"># 残差连接</span></span><br><span class="line">        src = <span class="variable language_">self</span>.norm1(src)</span><br><span class="line"></span><br><span class="line">        src2 = <span class="variable language_">self</span>.activation(<span class="variable language_">self</span>.linear1(src))</span><br><span class="line">        src2 = <span class="variable language_">self</span>.linear2(<span class="variable language_">self</span>.dropout(src2))</span><br><span class="line">        src = src + <span class="variable language_">self</span>.dropout2(src2)</span><br><span class="line">        src = <span class="variable language_">self</span>.norm2(src)</span><br><span class="line">        <span class="keyword">return</span> src  <span class="comment"># [src_len, batch_size, num_heads * kdim] &lt;==&gt; [src_len,batch_size,embed_dim]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>编码器实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEncoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder_layer, num_layers, norm=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.layers = nn.ModuleList([copy.deepcopy(encoder_layer) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)])</span><br><span class="line">        <span class="variable language_">self</span>.num_layers = num_layers</span><br><span class="line">        <span class="variable language_">self</span>.norm = norm</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, mask=<span class="literal">None</span>, src_key_padding_mask=<span class="literal">None</span></span>):</span><br><span class="line">        output = src</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> mod <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            output = mod(output, src_mask=mask,</span><br><span class="line">                         src_key_padding_mask=src_key_padding_mask)</span><br><span class="line">        <span class="comment"># 多个encoder layers层堆叠后的前向传播过程</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = <span class="variable language_">self</span>.norm(output)</span><br><span class="line">        <span class="keyword">return</span> output  <span class="comment"># [src_len, batch_size, num_heads * kdim] &lt;==&gt; [src_len,batch_size,embed_dim]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>解码层实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerDecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, nhead, dim_feedforward=<span class="number">2048</span>, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.self_attn = MultiHeadedAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout)</span><br><span class="line">        <span class="comment"># 解码部分输入序列之间的多头注意力（也就是论文结构图中的Masked Multi-head attention)</span></span><br><span class="line">        <span class="variable language_">self</span>.multihead_attn = MultiHeadedAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout)</span><br><span class="line">        <span class="comment"># 编码部分输出（memory）和解码部分之间的多头注意力机制。</span></span><br><span class="line">        <span class="variable language_">self</span>.linear1 = nn.Linear(d_model, dim_feedforward)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.linear2 = nn.Linear(dim_feedforward, d_model)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.norm1 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.norm2 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.norm3 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.dropout1 = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.dropout2 = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.dropout3 = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.activation = F.relu</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tgt, memory, tgt_mask=<span class="literal">None</span>, memory_mask=<span class="literal">None</span>, tgt_key_padding_mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                memory_key_padding_mask=<span class="literal">None</span></span>):</span><br><span class="line">        tgt2 = <span class="variable language_">self</span>.self_attn(tgt, tgt, tgt,</span><br><span class="line">                              attn_mask=tgt_mask,</span><br><span class="line">                              key_padding_mask=tgt_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 解码部分输入序列之间的多头注意力（也就是论文结构图中的Masked Multi-head attention)</span></span><br><span class="line">        tgt = tgt + <span class="variable language_">self</span>.dropout1(tgt2)  <span class="comment"># 接着是残差连接</span></span><br><span class="line">        tgt = <span class="variable language_">self</span>.norm1(tgt)</span><br><span class="line"></span><br><span class="line">        tgt2 = <span class="variable language_">self</span>.multihead_attn(tgt, memory, memory,</span><br><span class="line">                                   attn_mask=memory_mask,</span><br><span class="line">                                   key_padding_mask=memory_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 解码部分的输入经过多头注意力后同编码部分的输出（memory）通过多头注意力机制进行交互</span></span><br><span class="line">        tgt = tgt + <span class="variable language_">self</span>.dropout2(tgt2)  <span class="comment"># 残差连接</span></span><br><span class="line">        tgt = <span class="variable language_">self</span>.norm2(tgt)</span><br><span class="line"></span><br><span class="line">        tgt2 = <span class="variable language_">self</span>.activation(<span class="variable language_">self</span>.linear1(tgt))</span><br><span class="line">        tgt2 = <span class="variable language_">self</span>.linear2(<span class="variable language_">self</span>.dropout(tgt2))</span><br><span class="line">        <span class="comment"># 最后的两层全连接</span></span><br><span class="line">        tgt = tgt + <span class="variable language_">self</span>.dropout3(tgt2)</span><br><span class="line">        tgt = <span class="variable language_">self</span>.norm3(tgt)</span><br><span class="line">        <span class="keyword">return</span> tgt  <span class="comment"># [tgt_len, batch_size, num_heads * kdim] &lt;==&gt; [tgt_len,batch_size,embed_dim]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>解码器实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerDecoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, decoder_layer, num_layers, norm=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.layers = nn.ModuleList([copy.deepcopy(decoder_layer) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)])</span><br><span class="line">        <span class="variable language_">self</span>.num_layers = num_layers</span><br><span class="line">        <span class="variable language_">self</span>.norm = norm</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tgt, memory, tgt_mask=<span class="literal">None</span>, memory_mask=<span class="literal">None</span>, tgt_key_padding_mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                memory_key_padding_mask=<span class="literal">None</span></span>):</span><br><span class="line">        output = tgt</span><br><span class="line">        <span class="keyword">for</span> mod <span class="keyword">in</span> <span class="variable language_">self</span>.layers:  <span class="comment"># 这里的layers就是N层解码层堆叠起来的</span></span><br><span class="line">            output = mod(output, memory,</span><br><span class="line">                         tgt_mask=tgt_mask,</span><br><span class="line">                         memory_mask=memory_mask,</span><br><span class="line">                         tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                         memory_key_padding_mask=memory_key_padding_mask)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output = <span class="variable language_">self</span>.norm(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Transformer的网络结构实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model=<span class="number">512</span>, nhead=<span class="number">8</span>, num_encoder_layers=<span class="number">6</span>,</span></span><br><span class="line"><span class="params">                 num_decoder_layers=<span class="number">6</span>, dim_feedforward=<span class="number">2048</span>, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment">#  ================ 编码部分 =====================</span></span><br><span class="line">        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)</span><br><span class="line">        encoder_norm = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)</span><br><span class="line">        <span class="comment"># ================ 解码部分 =====================</span></span><br><span class="line">        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)</span><br><span class="line">        decoder_norm = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)</span><br><span class="line">        <span class="variable language_">self</span>._reset_parameters()  <span class="comment"># 初始化模型参数</span></span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model</span><br><span class="line">        <span class="variable language_">self</span>.nhead = nhead</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt, src_mask=<span class="literal">None</span>, tgt_mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                memory_mask=<span class="literal">None</span>, src_key_padding_mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                tgt_key_padding_mask=<span class="literal">None</span>, memory_key_padding_mask=<span class="literal">None</span></span>):</span><br><span class="line">        memory = <span class="variable language_">self</span>.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)</span><br><span class="line">        <span class="comment"># [src_len, batch_size, num_heads * kdim] &lt;==&gt; [src_len,batch_size,embed_dim]</span></span><br><span class="line">        output = <span class="variable language_">self</span>.decoder(tgt=tgt, memory=memory, tgt_mask=tgt_mask, memory_mask=memory_mask,</span><br><span class="line">                              tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                              memory_key_padding_mask=memory_key_padding_mask)</span><br><span class="line">        <span class="keyword">return</span> output  <span class="comment"># [tgt_len, batch_size, num_heads * kdim] &lt;==&gt; [tgt_len,batch_size,embed_dim]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>最后，我们实现参数初始化方法和注意力掩码矩阵生成方法</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_reset_parameters</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> <span class="variable language_">self</span>.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            xavier_uniform_(p)</span><br><span class="line">            </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_square_subsequent_mask</span>(<span class="params">self, sz</span>):</span><br><span class="line">    mask = (torch.triu(torch.ones(sz, sz)) == <span class="number">1</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    mask = mask.<span class="built_in">float</span>().masked_fill(mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)).masked_fill(mask == <span class="number">1</span>, <span class="built_in">float</span>(<span class="number">0.0</span>))</span><br><span class="line">    <span class="keyword">return</span> mask <span class="comment"># [sz,sz]</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p>到此，对于整个Transformer的网络结构就搭建完成了，我们可以编写代码进行测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    src_len = <span class="number">5</span></span><br><span class="line">    batch_size = <span class="number">2</span></span><br><span class="line">    dmodel = <span class="number">32</span></span><br><span class="line">    tgt_len = <span class="number">6</span></span><br><span class="line">    num_head = <span class="number">8</span></span><br><span class="line">    src = torch.rand((src_len, batch_size, dmodel))  <span class="comment"># shape: [src_len, batch_size, embed_dim]</span></span><br><span class="line">    src_key_padding_mask = torch.tensor([[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">                                         [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>]])  <span class="comment"># shape: [batch_size, src_len]</span></span><br><span class="line"></span><br><span class="line">    tgt = torch.rand((tgt_len, batch_size, dmodel))  <span class="comment"># shape: [tgt_len, batch_size, embed_dim]</span></span><br><span class="line">    tgt_key_padding_mask = torch.tensor([[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">                                         [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>]])  <span class="comment"># shape: [batch_size, tgt_len]</span></span><br><span class="line"></span><br><span class="line">    my_transformer = Transformer(d_model=dmodel, nhead=num_head, num_encoder_layers=<span class="number">6</span>,</span><br><span class="line">                                   num_decoder_layers=<span class="number">6</span>, dim_feedforward=<span class="number">500</span>)</span><br><span class="line">    tgt_mask = my_transformer.generate_square_subsequent_mask(tgt_len)</span><br><span class="line">    out = my_transformer(src=src, tgt=tgt, tgt_mask=tgt_mask,</span><br><span class="line">                         src_key_padding_mask=src_key_padding_mask,</span><br><span class="line">                         tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                         memory_key_padding_mask=src_key_padding_mask)</span><br><span class="line">    <span class="built_in">print</span>(out.shape)  <span class="comment"># torch.Size([6, 2, 32])if __name__ == &#x27;__main__&#x27;:</span></span><br><span class="line">    src_len = <span class="number">5</span></span><br><span class="line">    batch_size = <span class="number">2</span></span><br><span class="line">    dmodel = <span class="number">32</span></span><br><span class="line">    tgt_len = <span class="number">6</span></span><br><span class="line">    num_head = <span class="number">8</span></span><br><span class="line">    src = torch.rand((src_len, batch_size, dmodel))  <span class="comment"># shape: [src_len, batch_size, embed_dim]</span></span><br><span class="line">    src_key_padding_mask = torch.tensor([[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">                             [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>]])  <span class="comment"># shape: [batch_size, src_len]</span></span><br><span class="line"></span><br><span class="line">    tgt = torch.rand((tgt_len, batch_size, dmodel))  <span class="comment"># shape: [tgt_len, batch_size, embed_dim]</span></span><br><span class="line">    tgt_key_padding_mask = torch.tensor([[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">                      [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>]])  <span class="comment"># shape: [batch_size, tgt_len]</span></span><br><span class="line"></span><br><span class="line">    my_transformer = Transformer(d_model=dmodel, nhead=num_head, num_encoder_layers=<span class="number">6</span>,</span><br><span class="line">                                   num_decoder_layers=<span class="number">6</span>, dim_feedforward=<span class="number">500</span>)</span><br><span class="line">    tgt_mask = my_transformer.generate_square_subsequent_mask(tgt_len)</span><br><span class="line">    out = my_transformer(src=src, tgt=tgt, tgt_mask=tgt_mask,</span><br><span class="line">                         src_key_padding_mask=src_key_padding_mask,</span><br><span class="line">                         tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                         memory_key_padding_mask=src_key_padding_mask)</span><br><span class="line">    <span class="built_in">print</span>(out.shape) <span class="comment">#torch.Size([6, 2, 32])</span></span><br></pre></td></tr></table></figure>

</div></section><div id="reward-container"><span class="hty-icon-button button-glow" id="reward-button" title="打赏" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === &quot;none&quot;) ? &quot;block&quot; : &quot;none&quot;;"><span class="icon iconify" data-icon="ri:hand-coin-line"></span></span><div id="reward-comment">I'm so cute. Please give me money.</div></div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>Galaxy Ryan</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="https://galaxy-ryan.github.io/article/2024/10/13/transformer-design.html/" title="Transformer的网络架构实现">https://galaxy-ryan.github.io/article/2024/10/13/transformer-design.html/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><span class="icon iconify" data-icon="ri:creative-commons-line"></span><span class="icon iconify" data-icon="ri:creative-commons-by-line"></span><span class="icon iconify" data-icon="ri:creative-commons-nc-line"></span><span class="icon iconify" data-icon="ri:creative-commons-sa-line"></span></a> 许可协议。</li></ul></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/article/2024/10/16/ARIES.html/" rel="prev" title="ARIES-Recovery-Algorithm"><span class="icon iconify" data-icon="ri:arrow-left-s-line"></span><span class="post-nav-text">ARIES-Recovery-Algorithm</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/article/2024/10/11/15445-p4.html/" rel="next" title="CMU15445-Project4-学习心得与记录"><span class="post-nav-text">CMU15445-Project4-学习心得与记录</span><span class="icon iconify" data-icon="ri:arrow-right-s-line"></span></a></div></div></div><div class="hty-card" id="comment"></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2024 </span><span class="with-love" id="animate"><span class="icon iconify" data-icon="ri:cloud-line"></span></span><span class="author"> Galaxy Ryan</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v7.3.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.10.11</span></div><div class="live-time"><span>感谢陪伴</span><span id="display_live_time"></span><span class="moe-text">(●'◡'●)</span><script>function blog_live_time() {
  setTimeout(blog_live_time, 1000);
  const start = new Date('2024-01-26T00:00:00');
  const now = new Date();
  const timeDiff = (now.getTime() - start.getTime());
  const msPerMinute = 60 * 1000;
  const msPerHour = 60 * msPerMinute;
  const msPerDay = 24 * msPerHour;
  const passDay = Math.floor(timeDiff / msPerDay);
  const passHour = Math.floor((timeDiff % msPerDay) / 60 / 60 / 1000);
  const passMinute = Math.floor((timeDiff % msPerHour) / 60 / 1000);
  const passSecond = Math.floor((timeDiff % msPerMinute) / 1000);
  display_live_time.innerHTML = ` ${passDay} 天 ${passHour} 小时 ${passMinute} 分 ${passSecond} 秒`;
}
blog_live_time();
</script></div></footer></div><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><span class="icon iconify" data-icon="ri:arrow-up-s-line"></span><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a><script src="https://fastly.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script><script>const images = [...document.querySelectorAll('.markdown-body img')]
mediumZoom(images)</script><style>.medium-zoom-image {
  z-index: 99;
}</style></body></html>